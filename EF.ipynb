{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Setup & Libraries",
   "id": "2ca75cb02453c4f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 1: IMPORTS & CONFIGURATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PLOTTING SETUP FOR PUBLICATION\n",
    "# This makes fonts larger and lines thicker for academic papers\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "print(\"Libraries loaded. Ready for scientific computing.\")"
   ],
   "id": "7e7c6e6320ac1107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Data Loader",
   "id": "c24180c92ae4d2da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 2 (CORRECTED): DATA LOADING & CLEANING\n",
    "def load_and_clean_data(file_path):\n",
    "    print(\"Loading Dataset...\")\n",
    "\n",
    "    # 1. LOAD (Using 'tstamp' for date)\n",
    "    df = pd.read_csv(file_path, parse_dates=['tstamp'])\n",
    "\n",
    "    # 2. RENAME COLUMNS (To match our scientific code)\n",
    "    # We rename 'tstamp' -> 'Date' and 'PotActiva' -> 'PotAtiva'\n",
    "    df = df.rename(columns={'tstamp': 'Date', 'PotActiva': 'PotAtiva'})\n",
    "\n",
    "    # 3. SORTING\n",
    "    df = df.sort_values(by=['CPE', 'Date'])\n",
    "\n",
    "    # 4. REMOVE DUPLICATES\n",
    "    df = df.drop_duplicates(subset=['CPE', 'Date'], keep='first')\n",
    "\n",
    "    # 5. SCIENTIFIC INTERPOLATION\n",
    "    # Fill small gaps linearly to preserve data integrity\n",
    "    df['PotAtiva'] = df.groupby('CPE')['PotAtiva'].transform(\n",
    "        lambda x: x.interpolate(method='linear', limit=2)\n",
    "    )\n",
    "\n",
    "    # Drop rows that are still empty\n",
    "    df = df.dropna(subset=['PotAtiva'])\n",
    "\n",
    "    print(f\"Data Loaded Successfully.\")\n",
    "    print(f\"Unique Consumers: {df['CPE'].nunique()}\")\n",
    "    print(f\"Total Observations: {len(df)}\")\n",
    "    print(f\"Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# === ACTION ===\n",
    "# UPDATE THIS PATH to your actual file location\n",
    "filename = 'consumo15m_11_2025.csv'\n",
    "df_raw = load_and_clean_data(filename)"
   ],
   "id": "9b4b8e578682b1dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Engineering (The \"90% Accuracy\" Engine)",
   "id": "e0b0b809df3e8075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 3 (FINAL): COMPLETE FEATURE ENGINEERING\n",
    "def generate_research_features(df):\n",
    "    print(\"Processing Time-Series Features...\")\n",
    "\n",
    "    # 1. RESAMPLE TO HOURLY\n",
    "    df = df.set_index('Date')\n",
    "    df_hourly = df.groupby('CPE')['PotAtiva'].resample('1H').mean().reset_index()\n",
    "\n",
    "    # 2. FILL GAPS\n",
    "    df_hourly['PotAtiva'] = df_hourly.groupby('CPE')['PotAtiva'].transform(\n",
    "        lambda x: x.interpolate(method='linear', limit=24)\n",
    "    )\n",
    "\n",
    "    # 3. GENERATE TIME FEATURES\n",
    "    df_hourly['Hour'] = df_hourly['Date'].dt.hour\n",
    "    df_hourly['DayOfWeek'] = df_hourly['Date'].dt.dayofweek\n",
    "\n",
    "    # Cyclical Features (REQUIRED by Cell 6)\n",
    "    df_hourly['hour_sin'] = np.sin(2 * np.pi * df_hourly['Hour'] / 24)\n",
    "    df_hourly['hour_cos'] = np.cos(2 * np.pi * df_hourly['Hour'] / 24)\n",
    "    df_hourly['day_sin'] = np.sin(2 * np.pi * df_hourly['DayOfWeek'] / 7)\n",
    "    df_hourly['day_cos'] = np.cos(2 * np.pi * df_hourly['DayOfWeek'] / 7)\n",
    "\n",
    "    # 4. GENERATE LAG FEATURES (The Memory)\n",
    "    grouped = df_hourly.groupby('CPE')['PotAtiva']\n",
    "\n",
    "    df_hourly['lag_24h'] = grouped.shift(24)      # Yesterday\n",
    "    df_hourly['lag_168h'] = grouped.shift(168)    # Last Week\n",
    "\n",
    "    # 5. GENERATE ROLLING STATISTICS\n",
    "    # Rolling Mean of Last Week (Smooths out trends)\n",
    "    df_hourly['rolling_mean_7d'] = grouped.shift(168).rolling(window=24).mean()\n",
    "\n",
    "    # Rolling Volatility (Standard Deviation)\n",
    "    df_hourly['rolling_std_24h'] = grouped.shift(1).rolling(window=24).std()\n",
    "\n",
    "    # 6. FINAL CLEANUP\n",
    "    # Drop rows where features are calculated as NaN\n",
    "    df_clean = df_hourly.dropna(subset=['PotAtiva', 'lag_168h', 'rolling_mean_7d'])\n",
    "\n",
    "    print(f\"Features Generated. Final shape: {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# === ACTION ===\n",
    "df_features = generate_research_features(df_raw)"
   ],
   "id": "60a080cda47220ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clustering Methodology (Paper Figure 1)",
   "id": "7ddc3ed35716d0f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "   # CELL 4: DETERMINE OPTIMAL CLUSTERS (The \"Novelty\" Proof)\n",
    "def optimize_clusters(df):\n",
    "    print(\"Optimizing Clusters...\")\n",
    "\n",
    "    # 1. CREATE LOAD PROFILES\n",
    "    # Pivot: Rows=CPEs, Cols=Hours (0-23). We average usage per hour.\n",
    "    # This creates a \"Daily Signature\" for each user.\n",
    "    daily_profiles = df.groupby(['CPE', df['Date'].dt.hour])['PotAtiva'].mean().unstack()\n",
    "\n",
    "    # Normalize (So high users don't dominate low users just by volume)\n",
    "    scaler = MinMaxScaler()\n",
    "    daily_profiles_norm = pd.DataFrame(\n",
    "        scaler.fit_transform(daily_profiles.T).T,\n",
    "        index=daily_profiles.index,\n",
    "        columns=daily_profiles.columns\n",
    "    )\n",
    "\n",
    "    # 2. TEST K (2 to 6)\n",
    "    results = []\n",
    "    K_range = range(2, 7)\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(daily_profiles_norm)\n",
    "        score = silhouette_score(daily_profiles_norm, labels)\n",
    "        inertia = kmeans.inertia_\n",
    "        results.append({'k': k, 'silhouette': score, 'inertia': inertia})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 3. PLOT FOR PAPER\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Inertia (Elbow)\n",
    "    sns.lineplot(data=results_df, x='k', y='inertia', marker='o', ax=ax1, color='blue', label='Inertia')\n",
    "    ax1.set_ylabel('Inertia (Lower is Better)', color='blue')\n",
    "\n",
    "    # Silhouette (Quality)\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=results_df, x='k', y='silhouette', marker='s', ax=ax2, color='red', label='Silhouette')\n",
    "    ax2.set_ylabel('Silhouette Score (Higher is Better)', color='red')\n",
    "\n",
    "    plt.title('Cluster Optimization: Finding the \"Energy Phenotypes\"')\n",
    "    plt.show()\n",
    "\n",
    "    best_k = results_df.loc[results_df['silhouette'].idxmax()]['k']\n",
    "    print(f\"Mathematically Optimal Clusters (K): {int(best_k)}\")\n",
    "\n",
    "    return int(best_k), daily_profiles_norm\n",
    "\n",
    "# === ACTION ===\n",
    "best_k, profiles_norm = optimize_clusters(df_features)"
   ],
   "id": "1791755b13eefe1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 5: APPLY CLUSTERING LABELS\n",
    "# This assigns every consumer to either \"Cluster 0\" or \"Cluster 1\"\n",
    "def apply_clusters(df, k=2):\n",
    "    print(f\"Applying K-Means with K={k}...\")\n",
    "\n",
    "    # 1. Prepare Data (Same normalization as Cell 4)\n",
    "    daily_profiles = df.groupby(['CPE', df['Date'].dt.hour])['PotAtiva'].mean().unstack()\n",
    "    scaler = MinMaxScaler()\n",
    "    daily_profiles_norm = pd.DataFrame(\n",
    "        scaler.fit_transform(daily_profiles.T).T,\n",
    "        index=daily_profiles.index,\n",
    "        columns=daily_profiles.columns\n",
    "    )\n",
    "\n",
    "    # 2. Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(daily_profiles_norm)\n",
    "\n",
    "    # 3. Save Labels to the Main DataFrame\n",
    "    # Map the cluster label back to the original dataframe based on CPE\n",
    "    cluster_map = pd.Series(cluster_labels, index=daily_profiles.index)\n",
    "    df['Cluster'] = df['CPE'].map(cluster_map)\n",
    "\n",
    "    print(\"Cluster counts:\")\n",
    "    print(df.groupby('CPE')['Cluster'].first().value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "# === ACTION ===\n",
    "# Overwrite df_features with the version that has clusters\n",
    "df_final = apply_clusters(df_features, k=2)"
   ],
   "id": "52fb031e576ab838",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 6: GENERATE PAPER FIGURES\n",
    "def generate_paper_plots(df):\n",
    "    print(\"Generating Figures for Research Paper...\")\n",
    "\n",
    "    # Prepare Data\n",
    "    daily_profiles = df.groupby(['CPE', df['Date'].dt.hour])['PotAtiva'].mean().unstack()\n",
    "    scaler = MinMaxScaler()\n",
    "    daily_profiles_norm = pd.DataFrame(scaler.fit_transform(daily_profiles.T).T,\n",
    "                                     index=daily_profiles.index, columns=daily_profiles.columns)\n",
    "\n",
    "    # --- FIGURE 1: ELBOW PLOT ---\n",
    "    inertia = []\n",
    "    K_range = range(1, 10)\n",
    "    for k in K_range:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        km.fit(daily_profiles_norm)\n",
    "        inertia.append(km.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(K_range, inertia, 'bo-', linewidth=2)\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Figure 1: Elbow Method (Optimal K=2)')\n",
    "    plt.axvline(x=2, color='r', linestyle='--')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- FIGURE 2: CLUSTER PROFILES ---\n",
    "    km_final = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    km_final.fit(daily_profiles_norm)\n",
    "    centroids = km_final.cluster_centers_\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(centroids[0], color='blue', linewidth=3, label='Cluster 0 (Stable)')\n",
    "    plt.plot(centroids[1], color='orange', linewidth=3, label='Cluster 1 (Volatile)')\n",
    "    plt.title('Figure 2: Distinct Consumer Load Profiles')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Normalized Consumption')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# === ACTION ===\n",
    "generate_paper_plots(df_final)"
   ],
   "id": "f176f4db555f9d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL: PUBLICATION-QUALITY PCA SCATTER PLOT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_best_pca(df):\n",
    "    print(\"Generating Publication-Quality PCA Plot...\")\n",
    "\n",
    "    # 1. PREPARE DATA\n",
    "    # Ensure we have the hour column\n",
    "    if 'Hour' not in df.columns:\n",
    "        df['Hour'] = df['Date'].dt.hour\n",
    "\n",
    "    # Pivot: Rows=Consumers, Cols=Hours (The \"Shape\" of usage)\n",
    "    daily_profiles = df.groupby(['CPE', 'Hour'])['PotAtiva'].mean().unstack()\n",
    "\n",
    "    # Normalize (Critical for PCA)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(daily_profiles.T).T,\n",
    "                     index=daily_profiles.index,\n",
    "                     columns=daily_profiles.columns)\n",
    "\n",
    "    # Get Cluster Labels (Ensure they exist)\n",
    "    if 'Cluster' in df.columns:\n",
    "        # Create a map {CPE: ClusterLabel} to ensure alignment\n",
    "        cluster_map = df.groupby('CPE')['Cluster'].first()\n",
    "        labels = X.index.map(cluster_map)\n",
    "    else:\n",
    "        print(\"Error: Please run the clustering cell first to assign labels.\")\n",
    "        return\n",
    "\n",
    "    # 2. CALCULATE PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Calculate Variance Explained (Good for paper caption)\n",
    "    var_exp = pca.explained_variance_ratio_\n",
    "    total_var = sum(var_exp) * 100\n",
    "\n",
    "    # 3. CALCULATE TRUE CENTROIDS IN PCA SPACE\n",
    "    # (We find the center in 24D, then project it to 2D)\n",
    "    # This is more accurate than just averaging the dots\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    kmeans.fit(X) # Re-fit on aligned data to get centers\n",
    "    centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "    # 4. PLOT CONFIGURATION\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 7), dpi=300) # High DPI for printing\n",
    "\n",
    "    # Plot the dots (Consumers)\n",
    "    scatter = sns.scatterplot(\n",
    "        x=X_pca[:, 0],\n",
    "        y=X_pca[:, 1],\n",
    "        hue=labels,\n",
    "        palette=['#2b7bba', '#e67e22'], # Professional Blue & Orange\n",
    "        style=labels, # Different shapes for colorblind accessibility\n",
    "        s=100,\n",
    "        alpha=0.8,\n",
    "        edgecolor='w'\n",
    "    )\n",
    "\n",
    "    # Plot the Centroids (Red X)\n",
    "    plt.scatter(\n",
    "        centroids_pca[:, 0], centroids_pca[:, 1],\n",
    "        marker='X', s=400, linewidths=3,\n",
    "        color='red', label='Cluster Centroids', zorder=10\n",
    "    )\n",
    "\n",
    "    # 5. LABELS & LEGEND\n",
    "    plt.xlabel(f'Principal Component 1 ({var_exp[0]:.1%} Variance)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel(f'Principal Component 2 ({var_exp[1]:.1%} Variance)', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Consumer Segmentation Map (Total Explained Variance: {total_var:.1f}%)', fontsize=14, pad=15)\n",
    "\n",
    "    # Custom Legend\n",
    "    handles, _ = scatter.get_legend_handles_labels()\n",
    "    # Add the centroid marker to legend manually if needed, or rely on clear visual\n",
    "    plt.legend(title='Behavior Profile', loc='upper right', frameon=True, framealpha=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === ACTION ===\n",
    "plot_best_pca(df_final)"
   ],
   "id": "e9280a6e69f3b6b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Comparative Forecasting Loop",
   "id": "666413d8db222909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 6: SCIENTIFIC FORECASTING EXPERIMENT\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "def evaluate_performance(y_true, y_pred, name=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return {'Model': name, 'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "def run_experiment(df):\n",
    "    print(\"Initializing Scientific Forecasting Experiment...\")\n",
    "\n",
    "    # 1. TRAIN/TEST SPLIT (Time-Based)\n",
    "    # We test on the last 2 months of data to simulate \"Future Prediction\"\n",
    "    split_date = df['Date'].max() - pd.Timedelta(days=60)\n",
    "\n",
    "    train = df[df['Date'] < split_date]\n",
    "    test = df[df['Date'] >= split_date]\n",
    "\n",
    "    print(f\"Training on {len(train)} samples. Testing on {len(test)} samples.\")\n",
    "\n",
    "    # FEATURES (The ones we created in Cell 3)\n",
    "    features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "                'lag_24h', 'lag_168h', 'rolling_mean_7d', 'rolling_std_24h']\n",
    "    target = 'PotAtiva'\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ==========================================\n",
    "    # EXPERIMENT A: NAIVE BASELINE (Required)\n",
    "    # ==========================================\n",
    "    # Prediction = \"What happened same time last week?\"\n",
    "    baseline_mae = mean_absolute_error(test[target], test['lag_168h'])\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(test[target], test['lag_168h']))\n",
    "    results.append({'Model': 'Naive Baseline (Last Week)', 'MAE': baseline_mae, 'RMSE': baseline_rmse})\n",
    "\n",
    "    # ==========================================\n",
    "    # EXPERIMENT B: GLOBAL MODEL (Standard)\n",
    "    # ==========================================\n",
    "    print(\"\\nTraining Global XGBoost Model (All Clusters Mixed)...\")\n",
    "    global_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
    "    global_model.fit(train[features], train[target])\n",
    "\n",
    "    global_pred = global_model.predict(test[features])\n",
    "    results.append(evaluate_performance(test[target], global_pred, \"Global XGBoost\"))\n",
    "\n",
    "    # ==========================================\n",
    "    # EXPERIMENT C: CLUSTER-SPECIFIC MODELS (Novelty)\n",
    "    # ==========================================\n",
    "    print(\"\\nTraining Cluster-Specific Models (Your Contribution)...\")\n",
    "\n",
    "    # We store predictions here to calculate total error later\n",
    "    test_clustered = test.copy()\n",
    "    test_clustered['Hybrid_Pred'] = 0.0\n",
    "\n",
    "    # Loop through our 2 Clusters\n",
    "    for k in sorted(df['Cluster'].unique()):\n",
    "        print(f\"  > Training specialized model for Cluster {k}...\")\n",
    "\n",
    "        # Filter Data\n",
    "        k_train = train[train['Cluster'] == k]\n",
    "        k_test = test[test['Cluster'] == k]\n",
    "\n",
    "        if len(k_test) == 0:\n",
    "            continue\n",
    "\n",
    "        # Train specialized model\n",
    "        k_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
    "        k_model.fit(k_train[features], k_train[target])\n",
    "\n",
    "        # Predict\n",
    "        k_pred = k_model.predict(k_test[features])\n",
    "\n",
    "        # Save to main test set\n",
    "        test_clustered.loc[k_test.index, 'Hybrid_Pred'] = k_pred\n",
    "\n",
    "    # Evaluate the Combined Hybrid Approach\n",
    "    hybrid_mae = mean_absolute_error(test_clustered[target], test_clustered['Hybrid_Pred'])\n",
    "    hybrid_rmse = np.sqrt(mean_squared_error(test_clustered[target], test_clustered['Hybrid_Pred']))\n",
    "    results.append({'Model': 'Hybrid Cluster-XGBoost (Proposed)', 'MAE': hybrid_mae, 'RMSE': hybrid_rmse})\n",
    "\n",
    "    # ==========================================\n",
    "    # FINAL RESULTS TABLE\n",
    "    # ==========================================\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate % Improvement over Baseline\n",
    "    base_mae = results_df.loc[0, 'MAE']\n",
    "    results_df['% Improvement'] = 100 * (1 - (results_df['MAE'] / base_mae))\n",
    "\n",
    "    print(\"\\n=== FINAL RESEARCH RESULTS ===\")\n",
    "    print(results_df)\n",
    "\n",
    "    # VISUALIZATION (Figure 3 for Paper)\n",
    "    # Plot residuals (errors) to show the Hybrid model is tighter\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(test[target] - test['lag_168h'], label='Baseline Error', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(test_clustered[target] - test_clustered['Hybrid_Pred'], label='Hybrid Model Error', fill=True, alpha=0.3)\n",
    "    plt.xlim(-5, 5) # Zoom in on the center\n",
    "    plt.title(\"Error Distribution: Hybrid Model vs Baseline\")\n",
    "    plt.xlabel(\"Prediction Error (kW)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# === ACTION ===\n",
    "final_table = run_experiment(df_final)"
   ],
   "id": "ef5fd7da1bf92192",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CELL 8 (FIXED): MEMORY-SAFE MULTI-MODEL COMPARISON\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_high_accuracy_experiment(df):\n",
    "    print(\"Starting Optimized High-Accuracy Race...\")\n",
    "\n",
    "    # 1. OPTIMIZED SPLIT (Focus on recent relevant data)\n",
    "    # We take the last 14 months: 12 for training, 2 for testing.\n",
    "    # This reduces RAM usage by ~60% while keeping the most important patterns.\n",
    "    cutoff_date = df['Date'].max() - pd.Timedelta(days=400)\n",
    "    df_recent = df[df['Date'] > cutoff_date].copy()\n",
    "\n",
    "    split_date = df_recent['Date'].max() - pd.Timedelta(days=60)\n",
    "    train = df_recent[df_recent['Date'] < split_date]\n",
    "    test = df_recent[df_recent['Date'] >= split_date]\n",
    "\n",
    "    print(f\"Training on {len(train)} recent samples (Last 12 Months).\")\n",
    "\n",
    "    features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "                'lag_24h', 'lag_168h', 'rolling_mean_7d', 'rolling_std_24h']\n",
    "    target = 'PotAtiva'\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 2. DEFINE EFFICIENT MODELS (No memory-heavy Random Forest)\n",
    "    models = {\n",
    "        # LightGBM: The King of Efficiency\n",
    "        'LightGBM': lgb.LGBMRegressor(n_estimators=300, learning_rate=0.05,\n",
    "                                      num_leaves=31, verbose=-1, random_state=42, n_jobs=1),\n",
    "\n",
    "        # XGBoost: High Precision\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=300, learning_rate=0.05,\n",
    "                                    max_depth=6, n_jobs=1, random_state=42),\n",
    "\n",
    "        # HistGradientBoosting: Scikit-Learn's low-memory version of RF\n",
    "        'HistGradient': HistGradientBoostingRegressor(max_iter=300, learning_rate=0.05,\n",
    "                                                      random_state=42)\n",
    "    }\n",
    "\n",
    "    # 3. TRAIN & EVALUATE\n",
    "    trained_estimators = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(train[features], train[target])\n",
    "        pred = model.predict(test[features])\n",
    "\n",
    "        r2 = r2_score(test[target], pred)\n",
    "        mae = mean_absolute_error(test[target], pred)\n",
    "        print(f\"  > {name}: R2 = {r2:.4f}\")\n",
    "\n",
    "        results.append({'Model': name, 'R2 (Accuracy)': r2, 'MAE': mae})\n",
    "        trained_estimators.append((name, model))\n",
    "\n",
    "    # 4. ENSEMBLE (The \"Super Model\")\n",
    "    print(\"Training Ensemble (Voting) Model...\")\n",
    "    ensemble = VotingRegressor(estimators=trained_estimators, n_jobs=1)\n",
    "    ensemble.fit(train[features], train[target])\n",
    "    ens_pred = ensemble.predict(test[features])\n",
    "\n",
    "    ens_r2 = r2_score(test[target], ens_pred)\n",
    "    ens_mae = mean_absolute_error(test[target], ens_pred)\n",
    "    results.append({'Model': 'Ensemble (Combined)', 'R2 (Accuracy)': ens_r2, 'MAE': ens_mae})\n",
    "\n",
    "    # 5. OUTPUT TABLE\n",
    "    results_df = pd.DataFrame(results).sort_values(by='R2 (Accuracy)', ascending=False)\n",
    "    print(\"\\n=== FINAL ACCURACY REPORT ===\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 6. PLOT PREDICTIONS (Visual Proof)\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    # Plot a random 4-day window from the test set to show detail\n",
    "    sample_start = 0\n",
    "    sample_end = 96 # 4 days * 24 hours\n",
    "\n",
    "    plt.plot(test.iloc[sample_start:sample_end]['Date'],\n",
    "             test.iloc[sample_start:sample_end][target].values,\n",
    "             'k-', label='Actual Energy', linewidth=2)\n",
    "\n",
    "    plt.plot(test.iloc[sample_start:sample_end]['Date'],\n",
    "             ens_pred[sample_start:sample_end],\n",
    "             'r--', label='Ensemble Prediction', linewidth=2)\n",
    "\n",
    "    plt.title('Figure 3: Prediction vs Reality (Ensemble Model)')\n",
    "    plt.ylabel('Active Power (kW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# === ACTION ===\n",
    "run_high_accuracy_experiment(df_features)"
   ],
   "id": "2f25ce5512166eb7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
